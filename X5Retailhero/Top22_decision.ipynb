{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scikit-uplift","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting scikit-uplift\n  Downloading scikit_uplift-0.0.3-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from scikit-uplift) (0.25.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from scikit-uplift) (3.0.3)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.6/site-packages (from scikit-uplift) (1.18.1)\nRequirement already satisfied: scikit-learn>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from scikit-uplift) (0.22.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->scikit-uplift) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->scikit-uplift) (2019.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->scikit-uplift) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->scikit-uplift) (1.1.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->scikit-uplift) (2.4.6)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (0.14.1)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.4.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->scikit-uplift) (1.14.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->scikit-uplift) (45.2.0.post20200210)\nInstalling collected packages: scikit-uplift\nSuccessfully installed scikit-uplift-0.0.3\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport lightgbm as lgbm\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import auc\nfrom sklearn.utils.extmath import stable_cumsum\nfrom sklift.models import SoloModel, ClassTransformation, TwoModels\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clients = pd.read_csv('../input/x5-retail-hero/clients.csv', index_col='client_id',parse_dates=['first_issue_date','first_redeem_date'])\ndf_train = pd.read_csv('../input/x5-retail-hero/uplift_train.csv', index_col='client_id')\ndf_test = pd.read_csv('../input/x5-retail-hero/uplift_test.csv', index_col='client_id')\ndf_products = pd.read_csv('../input/x5-retail-hero/products.csv', index_col='product_id').reset_index()\ndf_purchases = pd.read_csv('../input/x5-retail-hero/purchases.csv',parse_dates=['transaction_datetime'],index_col='client_id')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge df_purchases and df_products\ndf_purchases.drop(columns=['express_points_received','express_points_spent','trn_sum_from_red'],inplace=True)\ndf_purchases = df_purchases.merge(df_products[['netto','is_alcohol','is_own_trademark','product_id']], left_on='product_id', right_on='product_id').set_index(df_purchases.index)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#uplift metrics\ndef uplift_score(prediction, treatment, target, rate=0.3):\n    order = np.argsort(-prediction)\n    treatment_n = int((treatment == 1).sum() * rate)\n    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n    control_n = int((treatment == 0).sum() * rate)\n    control_p = target[order][treatment[order] == 0][:control_n].mean()\n    score = treatment_p - control_p\n    return score\n\ndef get_score(valid_uplift,indices_valid,concat_train):\n    valid_score = uplift_score(\n        valid_uplift,\n        treatment=concat_train.iloc[indices_valid].treatment_flg.values,\n        target=concat_train.iloc[indices_valid].target.values,\n    )\n    return valid_score\n\ndef uplift_curve(valid_uplift,indices_valid,concat_train):\n    y_true = concat_train.iloc[indices_valid].target.values\n    treatment=concat_train.iloc[indices_valid].treatment_flg.values\n    uplift = valid_uplift\n    \n    y_true, uplift, treatment = np.array(y_true), np.array(uplift), np.array(treatment)\n    desc_score_indices = np.argsort(uplift, kind=\"mergesort\")[::-1]\n    y_true, uplift, treatment = y_true[desc_score_indices], uplift[desc_score_indices], treatment[desc_score_indices]\n\n    y_true_ctrl, y_true_trmnt = y_true.copy(), y_true.copy()\n\n    y_true_ctrl[treatment == 1] = 0\n    y_true_trmnt[treatment == 0] = 0\n\n    distinct_value_indices = np.where(np.diff(uplift))[0]\n    threshold_indices = np.r_[distinct_value_indices, uplift.size - 1]\n\n    num_trmnt = stable_cumsum(treatment)[threshold_indices]\n    y_trmnt = stable_cumsum(y_true_trmnt)[threshold_indices]\n\n    num_all = threshold_indices + 1\n\n    num_ctrl = num_all - num_trmnt\n    y_ctrl = stable_cumsum(y_true_ctrl)[threshold_indices]\n\n    curve_values = (np.divide(y_trmnt, num_trmnt, out=np.zeros_like(y_trmnt), where=num_trmnt != 0) -\\\n                    np.divide(y_ctrl, num_ctrl, out=np.zeros_like(y_ctrl), where=num_ctrl != 0)) * num_all\n\n    if num_all.size == 0 or curve_values[0] != 0 or num_all[0] != 0:\n        num_all = np.r_[0, num_all]\n        curve_values = np.r_[0, curve_values]\n\n    return num_all, curve_values\n\ndef auс_uplift(valid_uplift,indices_valid,concat_train):\n    y_true = concat_train.iloc[indices_valid].target.values\n    treatment=concat_train.iloc[indices_valid].treatment_flg.values\n    uplift = valid_uplift\n    \n    return auc(*uplift_curve(valid_uplift,indices_valid,concat_train))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#groupby sum,count,mean,nunique\ndf_purchases['trans_hour'] = df_purchases.transaction_datetime.dt.hour\ndf_purchases['dayofweek'] = df_purchases.transaction_datetime.dt.dayofweek\n\ngroup_by_7weeks = df_purchases[df_purchases['transaction_datetime'] > '2019-02-01'].groupby(['client_id','transaction_id'])\ngroup_by_2weeks = df_purchases[df_purchases['transaction_datetime'] > '2019-03-03'].groupby(['client_id','transaction_id'])\n\nlast_cols = ['regular_points_received','regular_points_spent', 'purchase_sum','store_id','dayofweek','trans_hour']\nall_hist = group_by_7weeks[last_cols].last()\ntwo_weeks = group_by_2weeks[last_cols].last()\n\nsum_cols = ['netto','is_alcohol','is_own_trademark']\nall_hist_sum = group_by_7weeks[sum_cols].sum()\ntwo_weeks_sum = group_by_2weeks[sum_cols].sum()\n\nmean_cols = ['product_quantity','netto','trn_sum_from_iss']\nall_hist_mean = group_by_7weeks[mean_cols].mean()\ntwo_weeks_mean = group_by_2weeks[mean_cols].mean()\n\nnu_cols = ['product_quantity','product_id']\nall_hist_nu = group_by_7weeks[nu_cols].nunique()\ntwo_weeks_nu = group_by_2weeks[nu_cols].nunique()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['regular_points_received','regular_points_spent', 'purchase_sum','dayofweek','trans_hour']","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features =  pd.concat([\n                    all_hist.groupby('client_id')['purchase_sum'].count(),\n                    two_weeks.groupby('client_id')['purchase_sum'].count(),\n                    all_hist.groupby('client_id')['dayofweek'].mean(),\n                    two_weeks.groupby('client_id')['dayofweek'].mean(),\n                    all_hist.groupby('client_id')['trans_hour'].mean(),\n                    two_weeks.groupby('client_id')['trans_hour'].mean(),\n                    all_hist.groupby('client_id')[['store_id']].nunique(),\n                    two_weeks.groupby('client_id')[['store_id']].nunique(),\n                    #mean\n                    all_hist_mean.groupby('client_id')['product_quantity'].mean(),\n                    two_weeks_mean.groupby('client_id')['product_quantity'].mean(),\n                    all_hist_mean.groupby('client_id')['netto'].mean(),\n                    two_weeks_mean.groupby('client_id')['netto'].mean(),\n                    all_hist_mean.groupby('client_id')['trn_sum_from_iss'].mean(),\n                    two_weeks_mean.groupby('client_id')['trn_sum_from_iss'].mean(),\n                    #nu\n                    all_hist_nu.groupby('client_id')['product_id'].mean(),\n                    two_weeks_nu.groupby('client_id')['product_id'].mean(),\n                    all_hist_nu.groupby('client_id')['product_id'].sum(),\n                    two_weeks_nu.groupby('client_id')['product_id'].sum(),\n                    all_hist_nu.groupby('client_id')['product_quantity'].mean(),\n                    two_weeks_nu.groupby('client_id')['product_quantity'].mean(),\n                    #sum\n                    all_hist_sum.groupby('client_id')['is_alcohol'].sum(),\n                    two_weeks_sum.groupby('client_id')['is_alcohol'].sum(),\n                    all_hist_sum.groupby('client_id')['is_own_trademark'].sum(),\n                    two_weeks_sum.groupby('client_id')['is_own_trademark'].sum(),\n                    #casual\n                    all_hist.groupby('client_id').sum(),\n                    two_weeks.groupby('client_id').sum()\n                      ],axis = 1)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.columns = ['total_count_purchase','two_weeks_count_purchase']+['total_mean_dayofweek','two_weeks_mean_dayofweek']+\\\n    ['total_mean_trans_hour','two_weeks_mean_trans_hour']+['total_count_store_id','two_weeks_count_store_id']+\\\n    ['total_mean_product_quantity','two_weeks_mean_product_quantity']+['total_mean_netto','two_weeks_mean_netto']+\\\n    ['total_mean_trn_sum_from_iss','two_weeks_mean_trn_sum_from_iss']+\\\n    ['total_nu_product_id','two_weeks_nu_product_id']+['total_sum_product_id','two_weeks_sum_product_id']+\\\n    ['total_nu_product_quantity','two_weeks_nu_product_quantity']+\\\n    ['total_sum_is_alcohol','two_weeks_sum_is_alcohol']+['total_sum_is_own_trademark','two_weeks_sum_is_own_trademark']+\\\n    list(c+\"_all\" for c in names)+list(c+\"_two_weeks\" for c in names)\n\nfeatures.drop(columns=['dayofweek_two_weeks','dayofweek_all','trans_hour_two_weeks','trans_hour_all'],inplace=True)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train = pd.concat([df_train,df_clients,features],axis = 1,sort = True)\nmerged_train = merged_train[~merged_train['target'].isnull()].copy()\n#features\nmerged_train['diff_quantity'] = merged_train['total_mean_product_quantity']-merged_train['two_weeks_mean_product_quantity']\nmerged_train['prop_alc'] = merged_train['total_sum_is_alcohol']/merged_train['total_sum_product_id']\nmerged_train['diff_total_points'] = merged_train['regular_points_received_all']-merged_train['regular_points_spent_all']\nmerged_train['diff_two_weeks_points'] = merged_train['regular_points_received_two_weeks']-merged_train['regular_points_spent_two_weeks']\nmerged_train['first_issue_date'] = merged_train['first_issue_date'].astype(int)/10**9\nmerged_train['first_redeem_date'] = merged_train['first_redeem_date'].astype(int)/10**9\nmerged_train['diff_time'] = merged_train['first_redeem_date']-merged_train['first_issue_date']\nmerged_train['gender'] = list(ord(v[0]) for v in merged_train['gender'].values)\n#category features\nmerged_train = merged_train.fillna(0)\nfor col in ['total_mean_trans_hour','two_weeks_mean_trans_hour','total_mean_dayofweek','two_weeks_mean_dayofweek']:\n    merged_train[col] = round(merged_train.total_mean_dayofweek).astype('int')\n    merged_train[col] = merged_train[col].astype('category')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill wrong age\nage_train = merged_train.loc[(merged_train.age>14) & (merged_train.age<100)]\nage_test = merged_train.loc[merged_train.age>100]\n\nage_params = {'learning_rate':0.01,'max_depth':6,'num_leaves':20,\n             'min_data_in_leaf':30, 'application':'binary',\n             'subsample':1, 'colsample_bytree': 0.8,\n             'reg_alpha':0.01,'data_random_seed':42,'metric':'binary_logloss'        \n                }\n\nX=merged_train.drop(columns=['age'])\ny=merged_train.age\n\nkf = KFold(n_splits=5, random_state=None, shuffle=False)\nkf.get_n_splits(X, y)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    age_model = lgbm.LGBMRegressor(**age_params)\n    age_model.fit(X_train,y_train)\n    age_predict = age_model.predict(X_test)\n    \nage_model.fit(X,y)\n#predict age\npredicted_age = age_model.predict(age_test.drop(columns=['age']))\nmerged_train.loc[age_test.index,'age'] = np.around(predicted_age, decimals=0)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def valid(concat_train):\n    final_auc_uplift_score=[]\n    final_uplift_score=[]\n    skf = StratifiedKFold(n_splits=5)\n    skf.get_n_splits(concat_train, concat_train.target)\n    \n    for indices_learn, indices_valid in skf.split(concat_train, concat_train.target):\n        params_t = {'learning_rate':0.01,'max_depth':6,'num_leaves':10,\n                     'min_data_in_leaf':10, 'application':'binary',\n                     'subsample':0.75, 'colsample_bytree': 0.8,\n                     'reg_alpha':0.5,'data_random_seed':12,'metric':'binary_logloss',\n                     'max_bin':450,'bagging_freq':2,'reg_lambda':0.5         \n            }\n        #fit\n        transformation_model1 = ClassTransformation(AdaBoostClassifier(n_estimators=30,base_estimator=RandomForestClassifier(max_depth=1)))\n        transformation_model1.fit(\n            concat_train.iloc[indices_learn,:].drop(columns=['treatment_flg','target']),\n            concat_train.iloc[indices_learn,:]['treatment_flg'].values,\n            concat_train.iloc[indices_learn,:].target)\n        transformation_model2 = ClassTransformation(lgbm.LGBMClassifier(**params_t))\n        transformation_model2.fit(\n            concat_train.iloc[indices_learn,:].drop(columns=['treatment_flg','target']),\n            concat_train.iloc[indices_learn,:]['treatment_flg'].values,\n            concat_train.iloc[indices_learn,:].target)\n        \n        #valid\n        X_valid = concat_train.iloc[indices_valid, :].drop(columns=['treatment_flg','target'])\n        #predict\n        predict_valid = (transformation_model1.predict(X_valid)+transformation_model2.predict(X_valid))/2\n        \n        print('AUC uplift score:', round(auс_uplift(predict_valid,indices_valid,concat_train)/10**7,3))\n        print('Right uplift score:', round(get_score(predict_valid,indices_valid,concat_train),4),'\\n')\n        \n        final_auc_uplift_score.append(auс_uplift(predict_valid,indices_valid,concat_train))\n        final_uplift_score.append(get_score(predict_valid,indices_valid,concat_train))  \n\n    print('final auc uplift score =',round(sum(final_auc_uplift_score)/len(final_auc_uplift_score)/10**7,3))\n    print('final uplift score =',sum(final_uplift_score)/len(final_uplift_score))\n    \n    return round(sum(final_auc_uplift_score)/len(final_auc_uplift_score)/10**7,3),sum(final_uplift_score)/len(final_uplift_score)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid(merged_train)","execution_count":null,"outputs":[{"output_type":"stream","text":"AUC uplift score: 3.734\nRight uplift score: 0.0738 \n\nAUC uplift score: 3.878\nRight uplift score: 0.0699 \n\nAUC uplift score: 4.633\nRight uplift score: 0.0949 \n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(concat_train):\n    x_names = list(concat_train.iloc[:,2:].columns)\n    diff = []\n    \n    for rs in range(1,2,1):\n        params_t = {'learning_rate':0.01,'max_depth':6,'num_leaves':10,\n                     'min_data_in_leaf':10, 'application':'binary',\n                     'subsample':0.75, 'colsample_bytree': 0.8,\n                     'reg_alpha':0.5,'data_random_seed':12,'metric':'binary_logloss',\n                     'max_bin':450,'bagging_freq':2,'reg_lambda':0.5         \n            }\n        #fit       \n        transformation_model1 = ClassTransformation(AdaBoostClassifier(n_estimators=30,base_estimator=RandomForestClassifier(max_depth=1)))\n        transformation_model1.fit(\n            concat_train.drop(columns=['treatment_flg','target']),\n            concat_train['treatment_flg'].values,\n            concat_train.target)\n        transformation_model2 = ClassTransformation(lgbm.LGBMClassifier(**params_t))\n        transformation_model2.fit(\n            concat_train.drop(columns=['treatment_flg','target']),\n            concat_train['treatment_flg'].values,\n            concat_train.target)\n\n        #test features\n        df_test['target'] = 1\n        merged_test = pd.concat([df_test,df_clients,features],axis = 1,sort = True)\n        merged_test = merged_test[~merged_test['target'].isnull()].copy()\n        merged_test['diff_quantity'] = merged_test['total_mean_product_quantity']-merged_test['two_weeks_mean_product_quantity']\n        merged_test['prop_alc'] = merged_test['total_sum_is_alcohol']/merged_test['total_sum_product_id']\n        merged_test['diff_total_points'] = merged_test['regular_points_received_all']-merged_test['regular_points_spent_all']\n        merged_test['diff_two_weeks_points'] = merged_test['regular_points_received_two_weeks']-\\\n                                                    merged_test['regular_points_spent_two_weeks']\n        merged_test['first_issue_date'] = merged_test['first_issue_date'].astype(int)/10**9\n        merged_test['first_redeem_date'] = merged_test['first_redeem_date'].astype(int)/10**9\n        merged_test['diff_time'] = merged_test['first_redeem_date']-merged_test['first_issue_date']\n        merged_test['gender'] = list(ord(v[0]) for v in merged_test['gender'].values)\n        merged_test = merged_test.fillna(0)\n        for col in ['total_mean_trans_hour','two_weeks_mean_trans_hour','total_mean_dayofweek','two_weeks_mean_dayofweek']:\n            merged_test[col] = round(merged_test.total_mean_dayofweek).astype('int')\n            merged_test[col] = merged_test[col].astype('category')\n            \n        test_x = merged_test[x_names].fillna(0)\n        predict_test = (transformation_model1.predict(test_x)+transformation_model2.predict(test_x))/2\n        diff.append(np.array(predict_test))\n    return sum(diff)/len(diff),test_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submit\nuplift_prediction, test_x = predict(merged_train.fillna(0))\ndf_submission = pd.DataFrame({'client_id':test_x.index.values,'uplift': uplift_prediction})\ndf_submission.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}